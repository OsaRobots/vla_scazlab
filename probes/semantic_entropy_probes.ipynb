{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import roc_auc_score, log_loss\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_best_split(s_entropy, \n",
    "                    plot=True, \n",
    "                    label=\"\",\n",
    "                    n_splits=100):\n",
    "    # find the best split threshold for the semantic entropy\n",
    "\n",
    "    splits = np.linspace(1e-10, s_entropy.max(), n_splits)\n",
    "    split_mses = []\n",
    "    \n",
    "    for split in splits:\n",
    "        low_idxs, high_idxs = s_entropy < split, s_entropy >= split\n",
    "        \n",
    "        if not any(low_idxs) or not any(high_idxs):\n",
    "            split_mses.append(float('inf'))\n",
    "            continue\n",
    "            \n",
    "        low_mean = np.mean(s_entropy[low_idxs])\n",
    "        high_mean = np.mean(s_entropy[high_idxs])\n",
    "        \n",
    "        mse = np.sum((s_entropy[low_idxs] - low_mean)**2) + np.sum((s_entropy[high_idxs] - high_mean)**2)\n",
    "        split_mses.append(mse)\n",
    "    \n",
    "    split_mses = np.array(split_mses)\n",
    "    best_split = splits[np.argmin(split_mses)]\n",
    "    \n",
    "    if plot:\n",
    "        plt.plot(splits, split_mses, label=label)\n",
    "        plt.xlabel('Split Thresholds')\n",
    "        plt.ylabel('Mean Squared Error')\n",
    "        plt.title('MSE vs Split Threshold')\n",
    "        if label:\n",
    "            plt.legend()\n",
    "    \n",
    "    return best_split\n",
    "\n",
    "def binarize_entropy(entropy, threshold):\n",
    "    # binarize the entropy based on threshold\n",
    "    return (entropy >= threshold).astype(int)\n",
    "\n",
    "def train_and_evaluate_model(X, y, test_size=0.2, random_state=42):\n",
    "    # train a logistic regression model to predict binarized entropy along an action dimension and evaluate it\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=test_size, random_state=random_state\n",
    "    )\n",
    "    \n",
    "    model = LogisticRegression(random_state=random_state)\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    y_pred_proba = model.predict_proba(X_test)\n",
    "    test_loss = log_loss(y_test, y_pred_proba)\n",
    "    test_accuracy = model.score(X_test, y_test)\n",
    "    test_auroc = roc_auc_score(y_test, y_pred_proba[:, 1])\n",
    "    \n",
    "    return model, test_accuracy, test_auroc, test_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def train_entropy_predictors(hidden_states_dict, entropies_dict):\n",
    "    # train entropy predictors for each action dimension\n",
    "    results = {}\n",
    "    \n",
    "    plt.figure(figsize=(15, 5))\n",
    "    \n",
    "    for action_dim in hidden_states_dict.keys():\n",
    "        print(f\"\\nProcessing action dimension {action_dim}\")\n",
    "        \n",
    "        X = hidden_states_dict[action_dim]\n",
    "        raw_entropy = entropies_dict[action_dim]\n",
    "        \n",
    "        split = find_best_split(raw_entropy, plot=True, label=f\"Action {action_dim}\")\n",
    "        binary_entropy = binarize_entropy(raw_entropy, split)\n",
    "        \n",
    "        model, accuracy, auroc, loss = train_and_evaluate_model(X, binary_entropy)\n",
    "        results[action_dim] = {\n",
    "            'model': model,\n",
    "            'split_threshold': split,\n",
    "            'test_accuracy': accuracy,\n",
    "            'test_auroc': auroc,\n",
    "            'test_loss': loss,\n",
    "            'high_entropy_ratio': np.mean(binary_entropy)\n",
    "        }\n",
    "        \n",
    "        print(f\"Action {action_dim} Results:\")\n",
    "        print(f\"Test Accuracy: {accuracy:.4f}\")\n",
    "        print(f\"Test AUROC: {auroc:.4f}\")\n",
    "        print(f\"Test Loss: {loss:.4f}\")\n",
    "        print(f\"High Entropy Ratio: {results[action_dim]['high_entropy_ratio']:.4f}\")\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return results\n",
    "\n",
    "\"\"\"\n",
    "# assumes in dictionaries like:\n",
    "hidden_states_dict = {\n",
    "    0: X_0,  # hidden states for action dimension 0\n",
    "    1: X_1,  # hidden states for action dimension 1\n",
    "    ...\n",
    "    6: X_6   # hidden states for action dimension 6\n",
    "}\n",
    "\n",
    "entropies_dict = {\n",
    "    0: y_0,  # semantic entropy or probability values for action dimension 0\n",
    "    1: y_1,  # semantic entropy or probability values for action dimension 1\n",
    "    ...\n",
    "    6: y_6   # semantic entropy or probability values for action dimension 6\n",
    "}\n",
    "\n",
    "# train models for all action dimensions\n",
    "results = train_entropy_predictors(hidden_states_dict, entropies_dict)\n",
    "\n",
    "# get an action specific trained model and metrics\n",
    "action_dim = 0\n",
    "model = results[action_dim]['model']\n",
    "accuracy = results[action_dim]['test_accuracy']\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_semantic_entropy(prob_vector, k):\n",
    "    # essentially an action level approximation of the trajectory level semantic entropy  \n",
    "    # principlied way of choosing k? probably something to do with dirichlet process...\n",
    "    def entropy(p):\n",
    "        return np.sum(-p * np.log(p + 1e-10))\n",
    "    \n",
    "    ses = []\n",
    "    for i in range(k):\n",
    "        split = prob_vector.shape[0] // (2 ** (i+1))\n",
    "        clustered_prob_vector = np.mean(prob_vector.reshape(-1, split), axis=1) # get normalized probabilities of belonging to a cluster for each cluster\n",
    "        ith_semantic_entropy = entropy(clustered_prob_vector) # entropy of the ith level of the hierarchy\n",
    "        ses.append(ith_semantic_entropy)\n",
    "    \n",
    "    return np.mean(np.array(ses))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.5270786 , 0.46039868, 0.54184972, 0.51950227])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random_arr = np.random.rand(256)\n",
    "split = random_arr.shape[0] // (2 ** 2)\n",
    "np.mean(random_arr.reshape(-1, split), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.684693768449274\n",
      "[0.49373864 0.530676  ]\n"
     ]
    }
   ],
   "source": [
    "def entropy(p):\n",
    "    return -np.sum(p * np.log(p + 1e-10))\n",
    "\n",
    "i=0\n",
    "split = random_arr.shape[0] // (2 ** (i+1))\n",
    "clustered_prob_vector = np.mean(random_arr.reshape(-1, split), axis=1)\n",
    "print(entropy(clustered_prob_vector))\n",
    "print(clustered_prob_vector)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "libero_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
