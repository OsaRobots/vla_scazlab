{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax.numpy as jnp\n",
    "import gp\n",
    "import jax\n",
    "import glob \n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from IPython.display import display\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_gp_classifier(X, y, params=None):\n",
    "    # predict binarized semantic entropy labels using gp classifier\n",
    "    # x should be (n_samples, hidden_dim) and y should be (n_samples,) binary labels\n",
    "    \n",
    "    #TODO: can probably just tune these hyperparams using MAP estimate or something like that \n",
    "    if params is None:\n",
    "        params = {\n",
    "            'constant': 0.0,\n",
    "            'signal_variance': 6.0,\n",
    "            'alpha_eps': 0.1,\n",
    "            'strength': 5.0,\n",
    "            'intercept_scaling': 1.0\n",
    "        }\n",
    "    \n",
    "    # gp funcs \n",
    "    mean_func = gp.constant_mean\n",
    "    cov_func = gp.cosine_kernel\n",
    "    \n",
    "    # negative log likelihood\n",
    "    nll = gp.beta_gp_nll(\n",
    "        mean_func=mean_func,\n",
    "        cov_func=cov_func,\n",
    "        params=params,\n",
    "        x_train=X,\n",
    "        y_train=y\n",
    "    )\n",
    "    \n",
    "    return {\n",
    "        'mean_func': mean_func,\n",
    "        'cov_func': cov_func,\n",
    "        'params': params,\n",
    "        'nll': nll\n",
    "    }\n",
    "\n",
    "def predict_entropy(model, X_test):\n",
    "    # predict entropy for test data\n",
    "    # X_test should be (n_samples, hidden_dim) and model should be the output of train_gp_classifier\n",
    "    predictions = gp.beta_gp_predict(\n",
    "        mean_func=model['mean_func'],\n",
    "        cov_func=model['cov_func'],\n",
    "        params=model['params'],\n",
    "        x_query=X_test,\n",
    "        var_only=True\n",
    "    )\n",
    "    \n",
    "    # get latent function since out of distribution\n",
    "    mu, var = gp.get_latent_gp(predictions)\n",
    "    \n",
    "    # probs and quantiles\n",
    "    probs, quantiles = gp.get_beta_quantiles(predictions, q=0.025)\n",
    "    \n",
    "    return {\n",
    "        'probabilities': probs,\n",
    "        'confidence_intervals': quantiles,\n",
    "        'latent_mean': mu,\n",
    "        'latent_var': var\n",
    "    }\n",
    "\n",
    "def train_all_action_dimensions(hidden_states_dict, entropies_dict):\n",
    "    # train the GP classifier for each action dimension\n",
    "    models = {}\n",
    "\n",
    "    for action_dim in hidden_states_dict.keys():\n",
    "        X = hidden_states_dict[action_dim]\n",
    "        y = entropies_dict[action_dim]\n",
    "        model = train_gp_classifier(X, y)\n",
    "        models[action_dim] = model\n",
    "    \n",
    "    return models \n",
    "\n",
    "\"\"\"\n",
    "# assumes in dictionaries like:\n",
    "hidden_states_dict = {\n",
    "    0: X_0,  # hidden states for action dimension 0\n",
    "    1: X_1,  # hidden states for action dimension 1\n",
    "    ...\n",
    "    6: X_6   # hidden states for action dimension 6\n",
    "}\n",
    "\n",
    "entropies_dict = {\n",
    "    0: y_0,  # semantic entropy or probability values for action dimension 0\n",
    "    1: y_1,  # semantic entropy or probability values for action dimension 1\n",
    "    ...\n",
    "    6: y_6   # semantic entropy or probability values for action dimension 6\n",
    "}\n",
    "\n",
    "# train models for all action dimensions\n",
    "results = train_all_action_dimensions(hidden_states_dict, entropies_dict)\n",
    "\n",
    "# TODO: metrics?\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vla_run_data = '/Users/jeremiahetiosaomeike/Desktop/Research_Projs/vla/LIBERO/probes/vla_run_data'\n",
    "npys = glob.glob(vla_run_data + '/*.npy')\n",
    "# dat = np.load(npys, allow_pickle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for num_trial in range(len(npys)):\n",
    "#     data_path = npys[num_trial]\n",
    "#     dat = np.load(data_path, allow_pickle=True)\n",
    "#     print(f'Trial: {num_trial}, Language Instruction: {dat[0][\"language_instruction\"]}')\n",
    "# print(dat[0]['language_instruction'])\n",
    "\n",
    "# group trials by language instruction\n",
    "grouped_trial_paths = {}\n",
    "for num_trial in range(len(npys)):\n",
    "    data_path = npys[num_trial]\n",
    "    dat = np.load(data_path, allow_pickle=True)\n",
    "    prompt = dat[0][\"language_instruction\"]\n",
    "\n",
    "    if prompt not in grouped_trial_paths.keys():\n",
    "        grouped_trial_paths[prompt] = [data_path]\n",
    "    else:\n",
    "        grouped_trial_paths[prompt].append(data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped_trial_paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for num_trial in range(len(npys)):\n",
    "    data_path = npys[num_trial]\n",
    "    dat = np.load(data_path, allow_pickle=True)\n",
    "    image = dat[0][\"image\"]\n",
    "    img = Image.fromarray(image)\n",
    "    display(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Focusing on trials 0, 1, 2 and 3 \n",
    "# img = Image.fromarray(dat[0]['image'])\n",
    "paths_white_can = grouped_trial_paths['lift the white can']\n",
    "images = []\n",
    "for f_path in paths_white_can:\n",
    "    dat = np.load(f_path, allow_pickle=True)\n",
    "    images.append(Image.fromarray(dat[0]['image']))\n",
    "\n",
    "for img in images:\n",
    "    display(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "paths = grouped_trial_paths['lift the red can']\n",
    "images = []\n",
    "for f_path in paths:\n",
    "    dat = np.load(f_path, allow_pickle=True)\n",
    "    images.append(Image.fromarray(dat[0]['image']))\n",
    "\n",
    "for img in images:\n",
    "    display(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lift_dr_pep_can_paths = grouped_trial_paths['lift the dr pepper can']\n",
    "images = []\n",
    "for f_path in paths:\n",
    "    dat = np.load(f_path, allow_pickle=True)\n",
    "    images.append(Image.fromarray(dat[0]['image']))\n",
    "\n",
    "print(paths)\n",
    "for img in images:\n",
    "    display(img)\n",
    "\n",
    "# first path in this has no coca cola can, second path does"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import probabilistic_probe\n",
    "\n",
    "datas = [np.load(path, allow_pickle=True) for path in lift_dr_pep_can_paths]\n",
    "hs_bg = [data[0]['hidden_state_before_gen'] for data in datas]\n",
    "hs_pg = [data[0]['hidden_state_post_gen'] for data in datas]\n",
    "params = gp.set_default_params({'alpha_eps': .1, 'strength': 5}, warp_func=None)\n",
    "\n",
    "query_bg = hs_bg[0]\n",
    "x_obs = np.asarray(hs_bg[1:]).squeeze(1)\n",
    "y_obs = np.array([0, 1, 1, 1]) # the second obs \n",
    "\n",
    "measures = probabilistic_probe.gpp(query_bg, x_obs, y_obs)\n",
    "measures\n",
    "# predictions = gp.beta_gp_predict(mean_func=gp.constant_mean, \n",
    "#                                  cov_func=gp.cosine_kernel,\n",
    "#                                  params=params,\n",
    "                                 \n",
    "#                                  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# query = second_hs_bg\n",
    "# x_obs = first_hs_bg \n",
    "# y_obs = np.array([0]) # the second obs \n",
    "\n",
    "# measures = probabilistic_probe.gpp(query, x_obs, y_obs)\n",
    "# measures\n",
    "x_obs = np.asarray(hs_bg[1:]).squeeze(1)\n",
    "x_obs.shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "libero_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
