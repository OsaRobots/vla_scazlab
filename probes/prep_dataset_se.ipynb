{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax.numpy as jnp\n",
    "import gp\n",
    "import jax\n",
    "import glob \n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from IPython.display import display\n",
    "import matplotlib.pyplot as plt\n",
    "import polars as pl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "vla_run_data = '/Users/jeremiahetiosaomeike/Desktop/Research_Projs/vla/LIBERO/probes/vla_run_data'\n",
    "npys = glob.glob(vla_run_data + '/*.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_semantic_entropy(prob_vector, k):\n",
    "    # essentially an action level approximation of the trajectory level semantic entropy  \n",
    "    # principlied way of choosing k? probably something to do with dirichlet process...\n",
    "    def entropy(p):\n",
    "        return np.sum(-p * np.log(p + 1e-10))\n",
    "    \n",
    "    ses = []\n",
    "    for i in range(k):\n",
    "        split = prob_vector.shape[0] // (2 ** (i+1))\n",
    "        clustered_prob_vector = np.mean(prob_vector.reshape(-1, split), axis=1) # get normalized probabilities of belonging to a cluster for each cluster\n",
    "        ith_semantic_entropy = entropy(clustered_prob_vector) # entropy of the ith level of the hierarchy\n",
    "        ses.append(ith_semantic_entropy)\n",
    "    \n",
    "    return np.mean(np.array(ses))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "test_dir = '/Users/jeremiahetiosaomeike/Desktop/Research_Projs/vla/LIBERO/probes/test_data'\n",
    "os.makedirs(test_dir)\n",
    "for num_trial in range(len(npys)):\n",
    "    data_path = npys[num_trial]\n",
    "    data = np.load(data_path, allow_pickle=True)\n",
    "    for dct in data:\n",
    "        ses = []\n",
    "        probs = dct['probs']\n",
    "        for prob_vec in probs:\n",
    "            se = get_semantic_entropy(prob_vec, k=1)\n",
    "            ses.append(se)\n",
    "        dct['se'] = ses\n",
    "    np.save(test_dir + f'/episode_test_{num_trial}.npy', data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_best_split(s_entropy, \n",
    "                    plot=True, \n",
    "                    label=\"\",\n",
    "                    n_splits=100):\n",
    "    # find the best split threshold for the semantic entropy\n",
    "\n",
    "    splits = np.linspace(1e-10, s_entropy.max(), n_splits)\n",
    "    split_mses = []\n",
    "    \n",
    "    for split in splits:\n",
    "        low_idxs, high_idxs = s_entropy < split, s_entropy >= split\n",
    "        \n",
    "        if not any(low_idxs) or not any(high_idxs):\n",
    "            split_mses.append(float('inf'))\n",
    "            continue\n",
    "            \n",
    "        low_mean = np.mean(s_entropy[low_idxs])\n",
    "        high_mean = np.mean(s_entropy[high_idxs])\n",
    "        \n",
    "        mse = np.sum((s_entropy[low_idxs] - low_mean)**2) + np.sum((s_entropy[high_idxs] - high_mean)**2)\n",
    "        split_mses.append(mse)\n",
    "    \n",
    "    split_mses = np.array(split_mses)\n",
    "    best_split = splits[np.argmin(split_mses)]\n",
    "    \n",
    "    if plot:\n",
    "        plt.plot(splits, split_mses, label=label)\n",
    "        plt.xlabel('Split Thresholds')\n",
    "        plt.ylabel('Mean Squared Error')\n",
    "        plt.title('MSE vs Split Threshold')\n",
    "        if label:\n",
    "            plt.legend()\n",
    "    \n",
    "    return best_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_npys = glob.glob(test_dir + '/*.npy')\n",
    "ses = []\n",
    "hs_bgs = []\n",
    "hs_pgs = []\n",
    "for num_trial in range(len(new_npys)):\n",
    "    data_path = new_npys[num_trial]\n",
    "    data = np.load(data_path, allow_pickle=True)\n",
    "    for dct in data:\n",
    "        hs_bgs.append(dct['hidden_state_before_gen'])\n",
    "        hs_pgs.append(dct['hidden_state_post_gen'])\n",
    "        ses.append(dct['se'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: Confusion matrix approach \n",
    "\n",
    "SUCCESS_TRIALS = {\n",
    "    0: 1, 1: 0, 2: 0, 3: 1, 4: 0, 5: 0, 6: 0, 7: 1, \n",
    "    8: 0, 9: 1, 10: 0, 11: 0, 12: 0, 13: 0, 14: 0, 15: 0,\n",
    "    16: 0, 17: 1, 18: 0, 19: 0, 20: 0, 21: 0, 22: 0, 23: 0,\n",
    "    24: 0, 25: 0, 26: 0, 27: 0, 28: 0, 29: 0, 30: 1, 31: 1,\n",
    "    32: 1, 33: 1, 34: 1, 35: 1, 36: 1, 37: 1, 38: 1, 39: 1\n",
    "}\n",
    "SUCCESS_TIMESTEPS = {\n",
    "    0: 23, 3: 21, 7: 23, 9: 25, 17: 27, 30: 32, 31: 41,\n",
    "    32: 25, 33: 38, 34: 24, 35: 31, 36: 22, 37: 21, 38: 16, 39: 16\n",
    "}\n",
    "\n",
    "ses = np.asarray(ses)\n",
    "hs_bgs = np.asarray(hs_bgs)\n",
    "hs_pgs = np.asarray(hs_pgs)\n",
    "thresholds = {}\n",
    "x_ses, y_ses, z_ses, rot_x_ses, rot_y_ses, rot_z_ses, done_ses = ses.T\n",
    "thresholds['x'] = find_best_split(x_ses, plot=False)\n",
    "thresholds['y'] = find_best_split(y_ses, plot=False)\n",
    "thresholds['z'] = find_best_split(z_ses, plot=False)\n",
    "thresholds['rot_x'] = find_best_split(rot_x_ses, plot=False)\n",
    "thresholds['rot_y'] = find_best_split(rot_y_ses, plot=False)\n",
    "thresholds['rot_z'] = find_best_split(rot_z_ses, plot=False)\n",
    "thresholds['done'] = find_best_split(done_ses, plot=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def binarize_s_entropy(s_entropy, threshold):\n",
    "    return (s_entropy >= threshold).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(64, 4096)\n",
      "(128, 4096)\n",
      "(128,)\n"
     ]
    }
   ],
   "source": [
    "num_trials = 10\n",
    "num_train = 128\n",
    "num_eval = 64\n",
    "random_idxs = np.random.choice(len(x_ses), num_train+num_eval, replace=False)\n",
    "\n",
    "# print(np.asarray(hs_bgs).squeeze(1).shape)\n",
    "# print(x_ses.shape)\n",
    "\n",
    "sampled_x_ses = x_ses[random_idxs]\n",
    "sampled_hs_bgs = np.asarray(hs_bgs).squeeze(1)[random_idxs]\n",
    "sampled_hs_pgs = np.asarray(hs_pgs).squeeze(1)[random_idxs]\n",
    "\n",
    "y_obs = binarize_s_entropy(sampled_x_ses[:num_train], thresholds['x'])\n",
    "x_obs_bgs = sampled_hs_bgs[:num_train]\n",
    "x_query = sampled_hs_bgs[num_train:]\n",
    "\n",
    "# print(x_query.shape)\n",
    "# print(x_obs_bgs.shape)\n",
    "# print(y_obs.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import probabilistic_probe\n",
    "\n",
    "measures = probabilistic_probe.gpp(x_query, x_obs_bgs, y_obs)\n",
    "measures"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "libero_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
